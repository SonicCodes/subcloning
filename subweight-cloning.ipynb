{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight subcloning https://arxiv.org/pdf/2312.09299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the base model (parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb\", name=\"CC-MAIN-2024-10\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup hooks to collect activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'resid': {},\n",
    "    'attn': {},\n",
    "    'out': {},\n",
    "    'in': {},\n",
    "    'main': {},\n",
    "    # 'interim': {}\n",
    "}\n",
    "\n",
    "def get_activation(name, layer_type, isinput):\n",
    "    def hook(model, input, output):\n",
    "        if name not in activations[layer_type]:\n",
    "            activations[layer_type][name] = []\n",
    "        if isinput == \"input\":\n",
    "            activations[layer_type][name].append(input[0].sum(dim=0).detach().cpu())\n",
    "        elif isinput == \"output2\":\n",
    "            activations[layer_type][name].append(output[1].var(dim=-1).sum(dim=0).detach().cpu())\n",
    "        elif isinput == \"output\":\n",
    "            activations[layer_type][name].append(output[0].sum(dim=0).detach().cpu())\n",
    "    return hook\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "hooks = []\n",
    "\n",
    "hooks.append(model.model.norm.register_forward_hook(get_activation('norm', 'main', 'output')))\n",
    "hooks.append(model.model.embed_tokens.register_forward_hook(get_activation('embed', 'main', 'output')))\n",
    "hooks.append(model.lm_head.register_forward_hook(get_activation('unembed', 'main', 'input')))\n",
    "\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    registerable_hooks = [\n",
    "        (layer.self_attn.o_proj, \"resid\" , 'oprj', \"output\"),\n",
    "        (layer.input_layernorm, \"resid\", 'inorm', \"output\"),\n",
    "        (layer.mlp.down_proj, \"resid\", 'dprj', \"output\"),\n",
    "        (layer.mlp.gate_proj, \"resid\", 'gprj', \"output\"),\n",
    "        (layer.self_attn, \"attn\", '', \"output2\"),\n",
    "\n",
    "    ]\n",
    "    def add_if_not_taken(module, type, name, layer_type):\n",
    "        # check by name + layer_type\n",
    "        if module == layer.self_attn:\n",
    "            return\n",
    "        registerable_hooks.append((module, type, name, layer_type))\n",
    "    for name, module in layer.named_modules():\n",
    "        if (\"self_attn\" in name):\n",
    "            continue\n",
    "        if len(name) == 0:\n",
    "            continue\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            add_if_not_taken(module, \"out\", name, \"output\")\n",
    "            add_if_not_taken(module, \"in\", name, \"input\")\n",
    "        if \"norm\" in name:\n",
    "            add_if_not_taken(module, \"out\", name, \"output\")\n",
    "\n",
    "    for module, type, name, layer_type in registerable_hooks:\n",
    "        hooks.append(module.register_forward_hook(get_activation(f'{i}_{name}', type, layer_type)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "def collect_activations(model, dataset, tokenizer, max_tokens=80_000, batch_size=12):\n",
    "    sequence_lengths = [512]  # Sequence lengths: 1, 2, 4, 8, ..., 2048\n",
    "    total_tokens = 0\n",
    "    batch_texts = []\n",
    "    batch_lengths = []\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    with tqdm.tqdm(total=max_tokens, desc='Collecting activations', unit='tokens') as progress:\n",
    "        for i,sample in enumerate(dataset):\n",
    "            # print(\"starting with sample: \", i+1, \" total tokens: \", total_tokens)\n",
    "            if total_tokens >= max_tokens:\n",
    "                print(\"Maximal tokens reached, total tokens: \", total_tokens)\n",
    "                break\n",
    "            # report which sample item we're in using tqdm report next to the progress bar\n",
    "            progress.set_postfix({\n",
    "                'sample': f\"{i+1}/211m\",\n",
    "            })\n",
    "\n",
    "            text = sample['text']\n",
    "            for seq_len in sequence_lengths:\n",
    "                if not (len(tokenizer.encode(text)) >= 512):\n",
    "                    continue\n",
    "                if random.random() > 0.9:\n",
    "                    continue\n",
    "\n",
    "         \n",
    "                batch_texts.append(text)\n",
    "                batch_lengths.append(seq_len)\n",
    "\n",
    "                # Process in batches\n",
    "                if len(batch_texts) >= batch_size:\n",
    "                    \n",
    "                    inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "                    total_tokens += inputs['input_ids'].numel()\n",
    "                    \n",
    "                    progress.update(inputs['input_ids'].numel())\n",
    "                    with torch.no_grad():\n",
    "                        out = model(**inputs, output_attentions=True)\n",
    "\n",
    "                    batch_texts = []\n",
    "                    batch_lengths = []\n",
    "                    del inputs\n",
    "                    del out\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "\n",
    "                if total_tokens >= max_tokens:\n",
    "                    print(\"Maximal tokens reached, total tokens: \", total_tokens)\n",
    "                    break\n",
    "\n",
    "        # Process any remaining texts in the batch\n",
    "        print(\"SOME TEXTS LEFT, NUMBER OF TEXTS LEFT: \", len(batch_texts))\n",
    "        \n",
    "    print(\"done with collecting activations, now concatenating them\")\n",
    "    # Convert activations to numpy arrays\n",
    "    for layer_type in activations:\n",
    "        for layer_index in activations[layer_type]:\n",
    "            activations[layer_type][layer_index] = torch.stack(activations[layer_type][layer_index], dim=0)\n",
    "\n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations:   8%|██████████████████▎                                                                                                                                                                                                                            | 6144/80000 [00:00<00:03, 23384.22tokens/s, sample=21/211m]LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Collecting activations: 86016tokens [00:31, 2705.56tokens/s, sample=380/211m]                                                                                                                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal tokens reached, total tokens:  86016\n",
      "Maximal tokens reached, total tokens:  86016\n",
      "SOME TEXTS LEFT, NUMBER OF TEXTS LEFT:  0\n",
      "done with collecting activations, now concatenating them\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "collected_activations = collect_activations(model, fw, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "layer_importance = {}\n",
    "for i in range(32):\n",
    "    layer_key = f'{i}'\n",
    "    if layer_key in activations['out']:\n",
    "        layer_activation = activations['out'][layer_key]\n",
    "        layer_importance[i] = torch.mean(torch.abs(layer_activation)).item()\n",
    "\n",
    "# Sort layers by importance\n",
    "sorted_layers = sorted(layer_importance.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct distillation network (child network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "import torch\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "config.num_hidden_layers = 8\n",
    "config.num_attention_heads = 8\n",
    "config.num_key_value_heads = 4\n",
    "config.hidden_size = 1024\n",
    "config.intermediate_size = 3584\n",
    "\n",
    "def get_model():\n",
    "    new_model = AutoModelForCausalLM.from_config(config).cuda()\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        new_model.config.pad_token_id = new_model.config.eos_token_id\n",
    "    return new_model\n",
    "new_model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclone weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "neuron_importancee = []\n",
    "def compute_global_neuron_importance(activations):\n",
    "    global_neuron_importance = torch.zeros(list(activations['resid'].values())[0].shape[-1])\n",
    "    for layer_key in activations['resid']:\n",
    "        layer_activations = activations['resid'][layer_key]\n",
    "        neuron_importancee.append(layer_activations.abs().mean(dim=(0, 1)))\n",
    "        global_neuron_importance += layer_activations.abs().mean(dim=(0, 1))\n",
    "    return global_neuron_importance\n",
    "\n",
    "\n",
    "def compute_head_importance(activations, num_heads):\n",
    "    head_importance = []\n",
    "    for layer_key in activations['attn']: \n",
    "        acts = activations['attn'][layer_key] # (B, H, S)\n",
    "        acts = acts.sum(dim=-1).mean(dim=0) # (B, H, S)\n",
    "        \n",
    "        head_importance.append(acts)\n",
    "\n",
    "\n",
    "    return torch.stack(head_importance, dim=0)\n",
    "\n",
    "def prepare_attention_weights(old_attn):\n",
    "    # Repeat K and V weights to match Q\n",
    "    num_kv_heads = old_attn.num_key_value_heads\n",
    "    repeat_factor = old_attn.num_heads // num_kv_heads\n",
    "    q_weights = old_attn.q_proj.weight.data.view(old_attn.num_heads, old_attn.head_dim, -1)\n",
    "    k_weights = old_attn.k_proj.weight.data.view(num_kv_heads, old_attn.head_dim, -1)\n",
    "    v_weights = old_attn.v_proj.weight.data.view(num_kv_heads, old_attn.head_dim, -1)\n",
    "    o_weights = old_attn.o_proj.weight.data.view(-1, old_attn.num_heads, old_attn.head_dim)\n",
    "\n",
    "    \n",
    "    return q_weights, k_weights, v_weights, o_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def subclone_attention(old_attn, new_attn, neuron_indices, head_indices):   \n",
    "    # print (new_attn.q_proj.weight.data.shape) \n",
    "    num_kv_heads = old_attn.num_key_value_heads\n",
    "    new_num_kv_heads = new_attn.num_key_value_heads\n",
    "    repeat_factor = old_attn.num_heads // num_kv_heads\n",
    "\n",
    "    q_weights, k_weights, v_weights, o_weights = prepare_attention_weights(old_attn)\n",
    "\n",
    "    kv_head_indices = (head_indices % num_kv_heads)[:new_num_kv_heads]\n",
    "\n",
    "    q_weights = q_weights[head_indices][:, :, neuron_indices]\n",
    "    k_weights = k_weights[kv_head_indices][:, :, neuron_indices]\n",
    "    v_weights = v_weights[kv_head_indices][:, :, neuron_indices]\n",
    "    o_weights = o_weights[neuron_indices][:, head_indices, :]\n",
    "\n",
    "    new_attn.q_proj.weight.data = q_weights.reshape(-1, len(neuron_indices)).float()\n",
    "    new_attn.k_proj.weight.data = k_weights.reshape(-1, len(neuron_indices)).float()\n",
    "    new_attn.v_proj.weight.data = v_weights.reshape(-1, len(neuron_indices)).float()\n",
    "    new_attn.o_proj.weight.data = o_weights.reshape(len(neuron_indices), -1).float()\n",
    "\n",
    "successful_weight_transfers = 0\n",
    "def subclone_weight(old_layer, new_layer, activations, sort_dim, pre_adjust=None):\n",
    "    global successful_weight_transfers\n",
    "    dim_k = new_layer.weight.data.shape[sort_dim]\n",
    "    neuron_importance = activations.abs()\n",
    "    neuron_importance = neuron_importance.sum(dim=[0])\n",
    "    if (neuron_importance.dim() > 1):\n",
    "        neuron_importance = neuron_importance.sum(dim=[0])\n",
    "    \n",
    "    sorted_neurons = torch.topk(neuron_importance, k=dim_k, sorted=False).indices\n",
    "    weight_data = old_layer.weight.data\n",
    "    \n",
    "    if sort_dim == 0:\n",
    "        new_layer.weight.data = weight_data[sorted_neurons].float()\n",
    "    elif sort_dim == 1:\n",
    "        if pre_adjust is not None:\n",
    "            new_layer.weight.data = pre_adjust(weight_data)[:, sorted_neurons].float()\n",
    "        else:\n",
    "            new_layer.weight.data = weight_data[:, sorted_neurons].float()\n",
    "    elif sort_dim == 2:\n",
    "        if pre_adjust is not None:\n",
    "            new_layer.weight.data = pre_adjust(weight_data)[:, :, sorted_neurons].float()\n",
    "        else:\n",
    "            new_layer.weight.data = weight_data[:, :, sorted_neurons].float()\n",
    "\n",
    "    if (weight_data.dim() > 1):\n",
    "        new_layer.weight.data *= ((weight_data.shape[-1]/new_layer.weight.data.shape[-1]) ** 0.5)\n",
    "    successful_weight_transfers += 1\n",
    "    \n",
    "\n",
    "def subclone_both_weight(old_layer, new_layer, outact, inact):\n",
    "    global successful_weight_transfers\n",
    "    new_input_dim = new_layer.weight.data.shape[1]\n",
    "    new_output_dim = new_layer.weight.data.shape[0]\n",
    "    def get_neuron_importance(activations, dim_k):\n",
    "        neuron_importance = activations.abs()\n",
    "        neuron_importance = neuron_importance.sum(dim=[0])\n",
    "        if (neuron_importance.dim() > 1):\n",
    "            neuron_importance = neuron_importance.sum(dim=[0])\n",
    "        \n",
    "        sorted_neurons = torch.topk(neuron_importance, k=dim_k, sorted=False).indices\n",
    "        return sorted_neurons\n",
    "\n",
    "    out_importance = get_neuron_importance(outact, new_output_dim)\n",
    "    in_importance = get_neuron_importance(inact, new_input_dim)\n",
    "    \n",
    "    new_layer.weight.data = old_layer.weight.data[out_importance][:, in_importance].float() \n",
    "    if (new_layer.weight.data.dim() > 1):\n",
    "        new_layer.weight.data *= ((old_layer.weight.data.shape[-1]/new_layer.weight.data.shape[-1]) ** 0.5)\n",
    "    successful_weight_transfers += 1\n",
    "    \n",
    "\n",
    "def subclone_layer(old_idx, activations,old_layer, new_layer, neuron_indices, head_indices, new_model):\n",
    "    subclone_attention(old_layer.self_attn, new_layer.self_attn, neuron_indices, head_indices)\n",
    "    \n",
    "\n",
    "    new_layer_named_modules = dict(new_layer.named_modules())\n",
    "\n",
    "    def pre_adjust_setup(name):\n",
    "        if \"down_proj\" in name:\n",
    "            return lambda x: x[neuron_indices, :]\n",
    "        elif \"gate_proj\" in name:\n",
    "            return lambda x: x[:, neuron_indices]\n",
    "        elif \"o_proj\" in name:\n",
    "            return lambda x: x[:, neuron_indices]\n",
    "        elif \"norm\" in name:\n",
    "            return lambda x: x[neuron_indices]\n",
    "        return None\n",
    "    \n",
    "    for name, module in old_layer.named_modules():\n",
    "        if len(name) == 0:\n",
    "            continue\n",
    "        if not ((isinstance(module, torch.nn.Linear)) or (\"norm\" in name)):\n",
    "            continue\n",
    "\n",
    "        new_module = new_layer_named_modules[name]\n",
    "\n",
    "        \n",
    "        if (f\"{old_idx}_{name}\" in activations[\"out\"] and f\"{old_idx}_{name}\" in activations[\"in\"]):\n",
    "\n",
    "            outact = activations[\"out\"][f\"{old_idx}_{name}\"]\n",
    "            inact = activations[\"in\"][f\"{old_idx}_{name}\"]\n",
    "            subclone_both_weight(module, new_module, outact, inact)\n",
    "\n",
    "        elif (f\"{old_idx}_{name}\" in activations[\"out\"]):\n",
    "      \n",
    "            outact = activations[\"out\"][f\"{old_idx}_{name}\"]\n",
    "            subclone_weight(module, new_module, outact, 0, pre_adjust=pre_adjust_setup(name))\n",
    "\n",
    "        elif (f\"{old_idx}_{name}\" in activations[\"in\"]):\n",
    "      \n",
    "            inact = activations[\"in\"][f\"{old_idx}_{name}\"]\n",
    "            subclone_weight(module, new_module, inact, 1, pre_adjust=pre_adjust_setup(name))\n",
    "        \n",
    "                \n",
    "    \n",
    "\n",
    "def subclone_model(base_model, new_model, activations):\n",
    "    if (\"subcloned_model\" in locals()):\n",
    "        del subcloned_model\n",
    "\n",
    "    global_neuron_importance = compute_global_neuron_importance(activations)\n",
    "\n",
    "    head_importance = compute_head_importance(activations, base_model.config.num_attention_heads)\n",
    "    \n",
    "    # Select top neurons and heads\n",
    "    top_neurons = torch.topk(global_neuron_importance, k=new_model.config.hidden_size, sorted=False).indices # global\n",
    "    top_heads = torch.topk(head_importance, k=new_model.config.num_attention_heads, sorted=False).indices # for each layer (layer, head)\n",
    "    \n",
    "    # Subclone embedding\n",
    "    subclone_weight(base_model.model.embed_tokens, new_model.model.embed_tokens, activations[\"main\"][\"embed\"], 1) # because it's embedding\n",
    "    subclone_weight(base_model.lm_head, new_model.lm_head, activations[\"main\"][\"unembed\"], 1)\n",
    "    subclone_weight(base_model.model.norm, new_model.model.norm, activations[\"main\"][\"norm\"], 0)\n",
    "    \n",
    "    # Subclone layers\n",
    "    layers_to_keep = [0, 1, 2,3, -4, -3, -2, -1]  # Adjust as needed\n",
    "    for new_idx, old_idx in enumerate(layers_to_keep):\n",
    "        if old_idx < 0:\n",
    "            old_idx = len(base_model.model.layers) + old_idx\n",
    "        subclone_layer(old_idx, activations ,base_model.model.layers[old_idx], new_model.model.layers[new_idx], top_neurons, top_heads[old_idx], new_model)\n",
    "    \n",
    "    \n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcloned_model = subclone_model(model, get_model(), collected_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 75)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successful_weight_transfers, sum(1 for i in subcloned_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "mismatch = []\n",
    "sucloned_params = dict(subcloned_model.named_parameters())\n",
    "for n,p in new_model.named_parameters():\n",
    "    if p.shape != sucloned_params[n].shape:\n",
    "        mismatch.append((n, p.shape, sucloned_params[n].shape))\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 375931904, memory needed: 1.40 GB, model norm: 6.601186669286108e-06\n",
      "Number of parameters: 375931904, memory needed: 1.40 GB, model norm: 6.537608442158671e-06\n",
      "Number of parameters: 8030261248, memory needed: 29.92 GB, model norm: 2.0712614059448242e-06\n"
     ]
    }
   ],
   "source": [
    "# number of params\n",
    "def count_parameters(model):\n",
    "    num_of_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model_norm = sum(p.norm() for p in model.parameters()) / num_of_params\n",
    "    memory_needed = num_of_params * 4 / 1024 / 1024 / 1024\n",
    "    print (f\"Number of parameters: {num_of_params}, memory needed: {memory_needed:.2f} GB, model norm: {model_norm}\")\n",
    "    \n",
    "count_parameters(new_model)\n",
    "count_parameters(subcloned_model)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check convergence of the subcloned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class StreamingDataset(IterableDataset):\n",
    "    def __init__(self, fw_dataset, tokenizer, max_length=512, count=None):\n",
    "        self.fw_dataset = fw_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.count = count\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in self.fw_dataset:\n",
    "            encoding = self.tokenizer(\n",
    "                item['text'],\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            yield {\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'labels': encoding['input_ids'].squeeze() # Shifted right by 1, original input as label\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count \n",
    "    \n",
    "def train(model, train_dataset, val_dataset, tokenizer, batch_size=4, epochs=6, lr=0.0001, max_grad_norm=1.0, warmup_steps=0, gradient_accumulation_steps=4):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.005)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * epochs)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            input_ids = batch['input_ids'].to(model.device).long()\n",
    "            attention_mask = batch['attention_mask'].to(model.device).float()\n",
    "            labels = batch['labels'].to(model.device).long()\n",
    "            \n",
    "            with autocast(dtype=torch.bfloat16):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Shift logits and labels for next token prediction\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), ignore_index=model.config.eos_token_id)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Step {step}/{len(train_dataloader)} | Loss: {total_loss / (step+1)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(model.device).long()\n",
    "                attention_mask = batch['attention_mask'].to(model.device)\n",
    "                labels = batch['labels'].to(model.device).long()\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "                \n",
    "                loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), ignore_index=model.config.eos_token_id)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Validation Loss: {val_loss}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), f\"model_checkpoint_epoch_{epoch+1}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your datasets\n",
    "train_dataset = StreamingDataset(fw.take(130_000_000).shuffle(), tokenizer, max_length=2048, count=130_000_000)\n",
    "val_dataset = StreamingDataset(fw.skip(130_000_000).take(10_000_000), tokenizer, max_length=2048, count=10_000_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcloned_model.gradient_checkpointing_enable()\n",
    "train(subcloned_model, train_dataset, val_dataset, tokenizer) # Subcloned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
